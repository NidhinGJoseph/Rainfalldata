# -*- coding: utf-8 -*-
"""RandomForest(RFD).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9eeSFNDhP-TsjGTS82COiadmtiUfRF4
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split, RandomizedSearchCV

"""Importing Libraries:

pandas: A powerful data manipulation library used for data analysis.

numpy: A library for numerical computations.

RandomForestRegressor: A machine learning model from scikit-learn used for regression tasks.

mean_absolute_error: A metric to evaluate the accuracy of regression models.

train_test_split: A utility to split the dataset into training and testing sets.

RandomizedSearchCV: A tool for hyperparameter optimization by searching through a subset of hyperparameter space.
"""

# Load the data from Excel
df = pd.read_excel('/content/Kurudamannil_RFD.xlsx')

"""Loading the Dataset"""

# Print column names to verify
print("Column names in the DataFrame:", df.columns)

# Strip whitespace from column names
df.columns = df.columns.str.strip()

"""*The code snippet starts by printing the column names of the DataFrame to verify their current state.

*It then strips any leading or trailing whitespace from the column names. This step is crucial for data cleaning, as it ensures that column names are standardized and free from formatting issues that could cause errors in data processing or analysis.






"""

# Convert the Date column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Create additional features from the date
df['DayOfYear'] = df['Date'].dt.dayofyear
df['Month'] = df['Date'].dt.month
df['DayOfMonth'] = df['Date'].dt.day

print(df.head())

"""The code snippet starts by converting the 'Date' column in the DataFrame to datetime objects, allowing for more advanced date-related operations.

It then creates three new features: 'DayOfYear', 'Month', and 'DayOfMonth', derived from the 'Date' column. These features can be useful for various analyses and modeling tasks.

Finally, it prints the first few rows of the DataFrame to provide a quick check on the transformations applied to the data.
"""

# Identify rows with missing values
missing_data = df[df['Rainfall'].isna()]

# Identify rows without missing values
non_missing_data = df[~df['Rainfall'].isna()]

# Drop rows with any NaNs from the training set
training_data = non_missing_data.dropna()

"""The code segment begins by identifying rows with missing values specifically in the 'Rainfall' column and storing them in a separate DataFrame missing_data.

It then creates another DataFrame non_missing_data containing rows without missing values in the 'Rainfall' column.

Finally, it drops any rows with missing values from the DataFrame non_missing_data to create the training dataset training_data.
"""

# Features and target variable for training
X_train_full = training_data[['DayOfYear', 'Month', 'DayOfMonth']]
y_train_full = training_data['Rainfall']

# Features for prediction
X_predict = missing_data[['DayOfYear', 'Month', 'DayOfMonth']]

"""The features selected for both training and prediction are 'DayOfYear', 'Month', and 'DayOfMonth', which were previously derived from the 'Date' column.

For training the model (X_train_full and y_train_full), only rows without missing values (training_data) are used to ensure data integrity.

For making predictions (X_predict), the features are extracted from the rows where 'Rainfall' values are missing (missing_data), implying that predictions will be made for these missing values.
"""

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=50, stop=200, num=4)],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2'],
    'bootstrap': [True, False]
}

"""param_dist: This dictionary contains the hyperparameters and their respective value ranges to be explored during the hyperparameter search.

n_estimators: The number of trees in the forest. It is chosen from a range of 50 to 200 with 4 evenly spaced values.

max_depth: The maximum depth of the tree. It includes options for no maximum depth (None) and depths of 10, 20, and 30.

min_samples_split: The minimum number of samples required to split an internal node. It includes options of 2, 5, and 10.

min_samples_leaf: The minimum number of samples required to be at a leaf node. It includes options of 1, 2, and 4.

max_features: The number of features to consider when looking for the best split. Options include 'auto', 'sqrt', and 'log2'.

bootstrap: Whether bootstrap samples are used when building trees. It includes options for True and False.

This parameter grid defines the space over which RandomizedSearchCV will search for the best combination of hyperparameters for the Random Forest Regressor model. Adjusting these hyperparameters can significantly impact the performance and generalization ability of the model.
"""

# Initialize the Random Forest Regressor
model = RandomForestRegressor(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1, scoring='neg_mean_absolute_error')

# Fit the RandomizedSearchCV to the data
random_search.fit(X_train_full, y_train_full)

# Print the best parameters
print("Best parameters found: ", random_search.best_params_)

"""model: The Random Forest Regressor model is initialized with a specified random state (for reproducibility).

random_search: The RandomizedSearchCV instance is initialized with the following parameters:

estimator: The model to be optimized, which is the Random Forest Regressor model in this case.
param_distributions: The parameter grid defined earlier, specifying the hyperparameters and their respective value ranges.
n_iter: The number of parameter settings that are sampled. In this case, 100 parameter settings will be sampled.
cv: The number of folds in cross-validation. Here, 5-fold cross-validation is used.
verbose: Controls the verbosity of the output during the search process. A higher value leads to more verbose output.
random_state: Controls the random seed for reproducibility.
n_jobs: The number of jobs to run in parallel. -1 means using all processors.
scoring: The scoring metric to optimize. Here, negative mean absolute error (neg_mean_absolute_error) is used.
random_search.fit(X_train_full, y_train_full): This line fits the RandomizedSearchCV instance to the training data (X_train_full and y_train_full), searching for the best combination of hyperparameters.

print("Best parameters found: ", random_search.best_params_): This line prints the best hyperparameters found during the search process.

This process helps identify the best hyperparameters for the Random Forest Regressor model, which can lead to improved performance and generalization on unseen data. The best hyperparameters are then printed for further analysis and model tuning.

The fitting process involved fitting the RandomizedSearchCV instance to the data, searching through 100 different combinations of hyperparameters for the Random Forest Regressor model. Each combination was evaluated using 5-fold cross-validation, resulting in a total of 500 fits.

After the fitting process, the best combination of hyperparameters was found to be:

'n_estimators': 150
'min_samples_split': 5
'min_samples_leaf': 4
'max_features': 'log2'
'max_depth': 10
'bootstrap': True
These hyperparameters are deemed optimal based on the negative mean absolute error metric (neg_mean_absolute_error) used for evaluation during the search.

This rigorous search for optimal hyperparameters ensures that the Random Forest Regressor model is fine-tuned for optimal performance and generalization on unseen data.
"""

# Use the best model to predict missing values
best_model = random_search.best_estimator_

missing_data.loc[:, 'Rainfall'] = best_model.predict(X_predict)

"""best_model: This variable stores the best estimator obtained from the RandomizedSearchCV process, which represents the Random Forest Regressor model with the optimal hyperparameters found during the search.

missing_data['Rainfall'] = best_model.predict(X_predict): This line uses the predict() method of the best model (best_model) to predict the missing values in the 'Rainfall' column of the missing_data DataFrame. The features for prediction (X_predict) are extracted from the rows where 'Rainfall' values are missing.

By employing the best model, the missing values in the 'Rainfall' column are estimated, completing the data imputation process. This ensures that the dataset is ready for further analysis or modeling without any missing values.
"""

# Predict the missing values
predicted_values = best_model.predict(X_predict)

# Calculate MAE
mae = mean_absolute_error(missing_data['Rainfall'], predicted_values)

print("Mean Absolute Error:", mae)

"""MAE of 0.9022 indicates a relatively good performance of the model in imputing missing values, but further analysis and comparison with alternative approaches may be necessary to fully assess its effectiveness.

Factors to be considered for checking the MAE (due to lack of subject knowledge)

The acceptable Mean Absolute Error (MAE) for imputing missing rainfall data depends on several factors specific to your context and the goals of your analysis. Here are some considerations to help determine a reasonable MAE for this task:

 1. **Domain Expertise and Historical Data**
- **Compare with Historical Variability:** Examine the historical variability of daily rainfall data. If the daily rainfall values typically vary by a small amount, a lower MAE would be expected. Conversely, if there is high variability, a higher MAE might be acceptable.
- **Consult Domain Experts:** Domain experts can provide insights into what constitutes an acceptable error margin for daily rainfall predictions.

 2. **Impact on Downstream Applications**
- **Agricultural Planning:** If the data is used for agricultural planning, the acceptable MAE should be low enough to ensure accurate irrigation schedules and crop management.
- **Flood Risk Assessment:** For flood risk assessment, the accuracy of extreme rainfall predictions might be more critical, so the MAE should reflect the need for precision in these scenarios.

 3. **Comparison with Baselines**
- **Simple Imputation Methods:** Compare the MAE of your model with simpler imputation methods, such as mean imputation or linear interpolation. Your model should ideally have a lower MAE than these baselines.
- **Existing Models:** Compare the MAE with other models or approaches used in similar studies or applications.

 4. **Contextual Factors**
- **Temporal Resolution:** If the data is aggregated (e.g., monthly instead of daily), the acceptable MAE might differ.
- **Geographical Variation:** Consider how rainfall patterns vary geographically. In areas with stable rainfall patterns, a lower MAE might be more feasible.

 5. **Model Performance Metrics**
- **Training vs. Validation MAE:** Ensure that the MAE for both training and validation sets are reasonable and close to each other, indicating good generalization.
- **Error Distribution:** Analyze the distribution of errors to ensure there are no significant biases or outliers.

 Practical Steps to Determine MAE:
1. **Historical Analysis:** Look at the historical standard deviation and mean of daily rainfall. An MAE significantly smaller than the standard deviation is generally good.
2. **Benchmark Models:** Use simpler models (e.g., mean or median imputation) as benchmarks. Your advanced model should outperform these.
3. **Expert Consultation:** Discuss with stakeholders what level of error is tolerable for their specific needs.

Example:
- Suppose the historical daily rainfall data has a mean of 10 mm and a standard deviation of 5 mm.
- If simpler imputation methods result in an MAE of 2-3 mm, then an advanced model with an MAE close to or below this range would be considered good.

Given these considerations, an MAE of around 0.9022, as mentioned earlier, seems quite reasonable, especially if historical variability is taken into account and if it outperforms simpler imputation methods. However, the final judgment should incorporate domain-specific requirements and the impact of imputation accuracy on subsequent analyses.
"""

# Combine the predicted values with the original data
df_imputed = pd.concat([non_missing_data, missing_data]).sort_values(by='Date')

# Optionally, save the imputed DataFrame back to Excel
df_imputed.to_excel('data_imputed.xlsx', index=False)
print("Imputed DataFrame:\n", df_imputed)

"""pd.concat([non_missing_data, missing_data]): Concatenates the DataFrame non_missing_data (containing original data with no missing values) and missing_data (containing original data with missing 'Rainfall' values replaced by predicted values) along the rows.

.sort_values(by='Date'): Sorts the combined DataFrame by the 'Date' column.

df_imputed.to_excel('data_imputed.xlsx', index=False): Optionally saves the combined DataFrame df_imputed to an Excel file named 'data_imputed.xlsx' without including the index column.

Prints the imputed DataFrame to the console for inspection.
This process merges the original dataset with the predicted 'Rainfall' values for missing data, creating a complete dataset ready for further analysis or modeling. Optionally, it saves the imputed DataFrame to an Excel file for future reference or sharing.
"""

# Evaluate the Model
# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

# Train the model on the training set
best_model.fit(X_train, y_train)

# Predictions on training set
train_predictions = best_model.predict(X_train)

# Predictions on validation set
val_predictions = best_model.predict(X_val)

# Calculate MAE for training set
train_mae = mean_absolute_error(y_train, train_predictions)

# Calculate MAE for validation set
val_mae = mean_absolute_error(y_val, val_predictions)

print("Training MAE:", train_mae)
print("Validation MAE:", val_mae)

"""1) Splits the full training data (X_train_full and y_train_full) into training and validation sets (X_train, X_val, y_train, y_val) with a test size of 20%.
The random state is set for reproducibility.

2) Trains the Random Forest Regressor model (best_model) on the training data (X_train, y_train).

3)Generates predictions for both the training set (train_predictions) and the validation set (val_predictions).

4)Computes the Mean Absolute Error (MAE) between the actual target values and the predicted values for both the training and validation sets.

5) Prints the MAE for both the training and validation sets to evaluate the performance of the model.
Output:

Training MAE: 9.281556900277671
Validation MAE: 10.381630202256124
These MAE values provide insights into how well the model generalizes to unseen data. A lower MAE indicates better performance, with the validation MAE serving as a proxy for the model's performance on new data. In this case, the model seems to perform slightly worse on the validation set compared to the training set, which is expected but still demonstrates reasonable performance.








"""